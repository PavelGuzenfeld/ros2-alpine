# Stage 1: Prepare a glibc-ready Alpine base
# This stage installs a full glibc environment into Alpine, which is a prerequisite
# for running the glibc-linked NVIDIA libraries and TensorFlow wheel.
FROM alpine:3.16 as builder_base

# Install essential tools and the glibc package from the sgerrand repository
ENV GLIBC_VERSION=2.35-r1
RUN apk add --no-cache \
        wget \
        ca-certificates && \
    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub && \
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-${GLIBC_VERSION}.apk && \
    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-bin-${GLIBC_VERSION}.apk && \
    apk add --no-cache \
        glibc-${GLIBC_VERSION}.apk \
        glibc-bin-${GLIBC_VERSION}.apk && \
    rm glibc-*.apk

# Stage 2: Download and extract minimal NVIDIA runtime libraries
# This stage uses an Ubuntu container to download NVIDIA's.deb packages
# and extract only the necessary.so files, avoiding a full toolkit installation.
FROM ubuntu:20.04 as downloader

# Add NVIDIA's CUDA repository
RUN apt-get update && apt-get install -y --no-install-recommends \
        gnupg \
        wget && \
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb && \
    dpkg -i cuda-keyring_1.0-1_all.deb && \
    apt-get update

# Create a directory to hold the extracted libraries
RUN mkdir -p /nvidia_libs

# Download (don't install) the required runtime packages for CUDA 11.4, cuDNN 8, TensorRT 8
# and extract their contents into the staging directory.
RUN apt-get download \
        libcublas-11-4 \
        libcudart-11-4 \
        libcudnn8 \
        libnvinfer8 \
        libnvinfer-plugin8 \
        libcusolver-11-4 \
        libcusparse-11-4 \
        libnvrtc-11-4 && \
    dpkg -x libcublas-11-4*.deb /nvidia_libs && \
    dpkg -x libcudart-11-4*.deb /nvidia_libs && \
    dpkg -x libcudnn8*.deb /nvidia_libs && \
    dpkg -x libnvinfer8*.deb /nvidia_libs && \
    dpkg -x libnvinfer-plugin8*.deb /nvidia_libs && \
    dpkg -x libcusolver-11-4*.deb /nvidia_libs && \
    dpkg -x libcusparse-11-4*.deb /nvidia_libs && \
    dpkg -x libnvrtc-11-4*.deb /nvidia_libs

# Stage 3: Build the Python environment and install TensorFlow
# This stage uses the glibc-ready base to install Python and all dependencies,
# including the special NVIDIA-provided TensorFlow wheel.
FROM builder_base as builder_tf

# Install Python and build dependencies
RUN apk add --no-cache \
        python3 \
        py3-pip \
        build-base \
        gfortran \
        libjpeg-turbo-dev \
        libpng-dev \
        zlib-dev

# Install Python package dependencies
RUN python3 -m pip install --upgrade pip && \
    pip3 install --no-cache-dir \
        numpy==1.21.1 \
        h5py==3.7.0 \
        setuptools==65.5.0 \
        testresources \
        future \
        mock \
        keras_preprocessing \
        keras_applications \
        gast==0.4.0 \
        protobuf \
        pybind11 \
        cython \
        pkgconfig \
        packaging

# Install the official NVIDIA TensorFlow wheel for JetPack 5.1 (L4T R35.2.1)
# The --extra-index-url is mandatory.
RUN pip3 install --no-cache-dir --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v51 tensorflow==2.11.0+nv23.1

# Stage 4: Final assembly of the minimal production image
FROM builder_base as final

# Install only the Python runtime, no build tools
RUN apk add --no-cache python3

# Copy the extracted NVIDIA libraries from the downloader stage
COPY --from=downloader /nvidia_libs/usr/local/cuda-11.4/lib64 /usr/local/nvidia/lib
COPY --from=downloader /nvidia_libs/usr/lib/aarch64-linux-gnu /usr/local/nvidia/lib

# Copy the installed Python packages from the builder_tf stage
COPY --from=builder_tf /usr/lib/python3.10/site-packages /usr/lib/python3.10/site-packages

# Set the library path so the system can find the NVIDIA libraries
ENV LD_LIBRARY_PATH=/usr/local/nvidia/lib

# Clean up apk cache
RUN rm -rf /var/cache/apk/*

# Set up a non-root user
RUN addgroup -S appgroup && adduser -S appuser -G appgroup
USER appuser
WORKDIR /home/appuser

# Define the default command
CMD ["python3"]